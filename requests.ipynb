{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"<insert_github_token_here>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Make a GET request to get repository information\n",
    "url = 'https://api.github.com/repos/ronaldozica/Projetos-JS'\n",
    "headers = {'Authorization': f'token {token}'}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    print(data)\n",
    "    # Use the data as needed\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the date as 'YYYY-MM-DD'\n",
    "since_date = '2023-01-01'\n",
    "formatted_since_date = datetime.strptime(since_date, '%Y-%m-%d').date().isoformat()\n",
    "\n",
    "# Set up the search parameters\n",
    "search_query = f'machine learning in:title stars:>50 pushed:>{formatted_since_date} sort:stars-desc is:issue'\n",
    "url = 'https://api.github.com/search/repositories'\n",
    "headers = {'Authorization': f'token {token}'}\n",
    "params = {'q': search_query}\n",
    "\n",
    "# Make a GET request to search for repositories\n",
    "response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    # Extract the list of repositories from the response\n",
    "    repositories = data['items']\n",
    "    \n",
    "    total_matches = data['total_count']\n",
    "    print(f\"Total Matches: {total_matches}\")\n",
    "    print(\"-------------\")\n",
    "\n",
    "    for repo in repositories:\n",
    "        print(f\"Repository Name: {repo['name']}\")\n",
    "        print(f\"Stars: {repo['stargazers_count']}\")\n",
    "        print(f\"Url: {repo['html_url']}\")\n",
    "        print(\"-------------\")\n",
    "\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "page_size = 100  # Set to the maximum value allowed by GitHub API\n",
    "\n",
    "# Define the date as 'YYYY-MM-DD'\n",
    "since_date = '2023-01-01'\n",
    "formatted_since_date = datetime.strptime(since_date, '%Y-%m-%d').date().isoformat()\n",
    "\n",
    "# Set up the search parameters\n",
    "search_query = f'machine learning in:title stars:>50 pushed:>{formatted_since_date} sort:stars-desc is:issue'\n",
    "url = 'https://api.github.com/search/repositories'\n",
    "headers = {'Authorization': f'token {token}'}\n",
    "\n",
    "# Make the initial request to get the total number of repositories\n",
    "params = {'q': search_query, 'per_page': 1, 'page': 1}\n",
    "response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    total_matches = data['total_count']\n",
    "    print(f\"Total Matches: {total_matches}\")\n",
    "    print(\"-------------\")\n",
    "\n",
    "    # Calculate the number of pages needed for pagination\n",
    "    total_pages = (total_matches // page_size) + 1\n",
    "\n",
    "    # Save information for each page\n",
    "    with open('repositories_info.txt', 'w') as file:\n",
    "        for page in range(1, total_pages + 1):\n",
    "            params = {'q': search_query, 'per_page': page_size, 'page': page}\n",
    "            response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                repositories = data['items']\n",
    "\n",
    "                for repo in repositories:\n",
    "                    repo_name = repo['name']\n",
    "                    stars_count = repo['stargazers_count']\n",
    "                    repo_url = repo['html_url']\n",
    "\n",
    "                    # Write repository information to the file\n",
    "                    file.write(f\"Repository Name: {repo_name}\\n\")\n",
    "                    file.write(f\"Stars: {stars_count}\\n\")\n",
    "                    file.write(f\"Url: {repo_url}\\n\")\n",
    "                    file.write(\"-------------\\n\")\n",
    "\n",
    "                    # Store additional information for making a GET request to the repository\n",
    "                    file.write(f\"Clone URL: {repo['clone_url']}\\n\")\n",
    "                    file.write(f\"Default Branch: {repo['default_branch']}\\n\")\n",
    "                    file.write(f\"Owner: {repo['owner']['login']}\\n\")\n",
    "                    file.write(\"===================================\\n\")\n",
    "\n",
    "            else:\n",
    "                print(f\"Request failed with status code {response.status_code}: {response.text}\")\n",
    "\n",
    "    print(\"Information saved to 'repositories_info.txt'\")\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "\n",
    "# Read repository information from file\n",
    "with open('repositories_info.txt', 'r') as file:\n",
    "    repo_info = file.read()\n",
    "\n",
    "# Split repository information into individual repositories\n",
    "repos = repo_info.split('===================================')\n",
    "\n",
    "# Remove empty strings from the list\n",
    "repos = [repo.strip() for repo in repos if repo.strip()]\n",
    "\n",
    "# GitHub API endpoint for retrieving README content\n",
    "readme_endpoint = 'https://api.github.com/repos/{owner_and_repo}/readme'\n",
    "\n",
    "# GitHub API headers\n",
    "headers = {'Authorization': f'token {token}'}\n",
    "\n",
    "# Keywords to check for in the README content\n",
    "keywords_to_check = ['demo', 'lesson', 'quiz', 'study', 'student']\n",
    "\n",
    "# Output file\n",
    "output_file_path = 'output.txt'\n",
    "\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    for repo in repos:\n",
    "        # Extract repository information\n",
    "        repo_name = repo.split('\\n')[0].split(': ')[1].strip()\n",
    "        owner = repo.split('\\n')[-2].split(': ')[1].strip()\n",
    "        clone_url = repo.split('\\n')[4].split(': ')[1].strip()\n",
    "\n",
    "        # Remove \".git\" extension from clone URL\n",
    "        clone_url = clone_url.replace('.git', '')\n",
    "\n",
    "        # Extract owner and repository name from the clone URL\n",
    "        owner_and_repo = \"/\".join(clone_url.split('/')[-2:])\n",
    "\n",
    "        # Make a GET request to get README information\n",
    "        readme_url = readme_endpoint.format(owner_and_repo=owner_and_repo)\n",
    "        response = requests.get(readme_url, headers=headers)\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            readme_data = response.json()\n",
    "            # Access the README content using base64 decoding\n",
    "            readme_content = base64.b64decode(readme_data['content']).decode('utf-8')\n",
    "\n",
    "            # Check for the presence of keywords\n",
    "            present_keywords = [keyword for keyword in keywords_to_check if keyword in readme_content.lower()]\n",
    "\n",
    "            # Print repository information to the console\n",
    "            output_message = f\"Repository: {repo_name}\\nClone URL: {clone_url}\\n\"\n",
    "            if present_keywords:\n",
    "                output_message += f\"Keywords Present: {', '.join(present_keywords)}\\n\"\n",
    "            else:\n",
    "                output_message += \"No keywords found in README\\n\"\n",
    "            output_message += '=' * 50 + '\\n'\n",
    "            \n",
    "            print(output_message)\n",
    "            \n",
    "            # Write repository information to the output file\n",
    "            output_file.write(output_message)\n",
    "        elif response.status_code == 404:\n",
    "            print(f\"README not found for {repo_name}\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve README for {repo_name}: {response.text}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
