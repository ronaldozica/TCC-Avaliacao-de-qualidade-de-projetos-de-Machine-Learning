{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install python-decouple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config\n",
    "\n",
    "# Load environment variables from .env file\n",
    "token = config('GIT_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Make a GET request to get repository information\n",
    "url = 'https://api.github.com/repos/ronaldozica/Projetos-JS'\n",
    "headers = {'Authorization': f'token {token}'}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    print(data)\n",
    "    # Use the data as needed\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the date as 'YYYY-MM-DD'\n",
    "since_date = '2023-01-01'\n",
    "formatted_since_date = datetime.strptime(since_date, '%Y-%m-%d').date().isoformat()\n",
    "\n",
    "# Set up the search parameters\n",
    "search_query = f'machine learning in:title stars:>50 pushed:>{formatted_since_date} sort:stars-desc is:issue'\n",
    "url = 'https://api.github.com/search/repositories'\n",
    "headers = {'Authorization': f'token {token}'}\n",
    "params = {'q': search_query}\n",
    "\n",
    "# Make a GET request to search for repositories\n",
    "response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    # Extract the list of repositories from the response\n",
    "    repositories = data['items']\n",
    "    \n",
    "    total_matches = data['total_count']\n",
    "    print(f\"Total Matches: {total_matches}\")\n",
    "    print(\"-------------\")\n",
    "\n",
    "    for repo in repositories:\n",
    "        print(f\"Repository Name: {repo['name']}\")\n",
    "        print(f\"Stars: {repo['stargazers_count']}\")\n",
    "        print(f\"Url: {repo['html_url']}\")\n",
    "        print(\"-------------\")\n",
    "\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "page_size = 100  # Set to the maximum value allowed by GitHub API\n",
    "\n",
    "# Define the date as 'YYYY-MM-DD'\n",
    "since_date = '2023-01-01'\n",
    "formatted_since_date = datetime.strptime(since_date, '%Y-%m-%d').date().isoformat()\n",
    "\n",
    "# Set up the search parameters\n",
    "search_query = f'machine learning in:title stars:>50 pushed:>{formatted_since_date} sort:stars-desc is:issue'\n",
    "url = 'https://api.github.com/search/repositories'\n",
    "headers = {'Authorization': f'token {token}'}\n",
    "\n",
    "# Make the initial request to get the total number of repositories\n",
    "params = {'q': search_query, 'per_page': 1, 'page': 1}\n",
    "response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    total_matches = data['total_count']\n",
    "    print(f\"Total Matches: {total_matches}\")\n",
    "    print(\"-------------\")\n",
    "\n",
    "    # Calculate the number of pages needed for pagination\n",
    "    total_pages = (total_matches // page_size) + 1\n",
    "\n",
    "    # Save information for each page\n",
    "    with open('repositories_info.txt', 'w') as file:\n",
    "        for page in range(1, total_pages + 1):\n",
    "            params = {'q': search_query, 'per_page': page_size, 'page': page}\n",
    "            response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                repositories = data['items']\n",
    "\n",
    "                for repo in repositories:\n",
    "                    repo_name = repo['name']\n",
    "                    stars_count = repo['stargazers_count']\n",
    "                    repo_url = repo['html_url']\n",
    "\n",
    "                    # Write repository information to the file\n",
    "                    file.write(f\"Repository Name: {repo_name}\\n\")\n",
    "                    file.write(f\"Stars: {stars_count}\\n\")\n",
    "                    file.write(f\"Url: {repo_url}\\n\")\n",
    "                    file.write(\"-------------\\n\")\n",
    "\n",
    "                    # Store additional information for making a GET request to the repository\n",
    "                    file.write(f\"Clone URL: {repo['clone_url']}\\n\")\n",
    "                    file.write(f\"Default Branch: {repo['default_branch']}\\n\")\n",
    "                    file.write(f\"Owner: {repo['owner']['login']}\\n\")\n",
    "                    file.write(\"===================================\\n\")\n",
    "\n",
    "            else:\n",
    "                print(f\"Request failed with status code {response.status_code}: {response.text}\")\n",
    "\n",
    "    print(\"Information saved to 'repositories_info.txt'\")\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "\n",
    "# Read repository information from file\n",
    "with open('repositories_info.txt', 'r') as file:\n",
    "    repo_info = file.read()\n",
    "\n",
    "# Split repository information into individual repositories\n",
    "repos = repo_info.split('===================================')\n",
    "\n",
    "# Remove empty strings from the list\n",
    "repos = [repo.strip() for repo in repos if repo.strip()]\n",
    "\n",
    "# GitHub API endpoint for retrieving README content\n",
    "readme_endpoint = 'https://api.github.com/repos/{owner_and_repo}/readme'\n",
    "\n",
    "# GitHub API headers\n",
    "headers = {'Authorization': f'token {token}'}\n",
    "\n",
    "# Keywords to check for in the README content\n",
    "keywords_to_check = ['demo', 'lesson', 'quiz', 'study', 'student']\n",
    "\n",
    "# Output file\n",
    "output_file_path = 'output.txt'\n",
    "\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    for repo in repos:\n",
    "        # Extract repository information\n",
    "        repo_name = repo.split('\\n')[0].split(': ')[1].strip()\n",
    "        owner = repo.split('\\n')[-2].split(': ')[1].strip()\n",
    "        clone_url = repo.split('\\n')[4].split(': ')[1].strip()\n",
    "\n",
    "        # Remove \".git\" extension from clone URL\n",
    "        clone_url = clone_url.replace('.git', '')\n",
    "\n",
    "        # Extract owner and repository name from the clone URL\n",
    "        owner_and_repo = \"/\".join(clone_url.split('/')[-2:])\n",
    "\n",
    "        # Make a GET request to get README information\n",
    "        readme_url = readme_endpoint.format(owner_and_repo=owner_and_repo)\n",
    "        response = requests.get(readme_url, headers=headers)\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            readme_data = response.json()\n",
    "            # Access the README content using base64 decoding\n",
    "            readme_content = base64.b64decode(readme_data['content']).decode('utf-8')\n",
    "\n",
    "            # Check for the presence of keywords\n",
    "            present_keywords = [keyword for keyword in keywords_to_check if keyword in readme_content.lower()]\n",
    "\n",
    "            # Print repository information to the console\n",
    "            output_message = f\"Repository: {repo_name}\\nClone URL: {clone_url}\\n\"\n",
    "            if present_keywords:\n",
    "                output_message += f\"Keywords Present: {', '.join(present_keywords)}\\n\"\n",
    "            else:\n",
    "                output_message += \"No keywords found in README\\n\"\n",
    "            output_message += '=' * 50 + '\\n'\n",
    "            \n",
    "            print(output_message)\n",
    "            \n",
    "            # Write repository information to the output file\n",
    "            output_file.write(output_message)\n",
    "        elif response.status_code == 404:\n",
    "            print(f\"README not found for {repo_name}\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve README for {repo_name}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = \"keywords_applied.txt\"\n",
    "output_file_path = \"filtered_repos.txt\"\n",
    "\n",
    "with open(input_file_path, 'r') as input_file, open(output_file_path, 'w') as output_file:\n",
    "    repository_lines = []\n",
    "    in_repository_block = False\n",
    "\n",
    "    for line in input_file:\n",
    "        line = line.strip()\n",
    "\n",
    "        if line.startswith(\"Repository:\"):\n",
    "            in_repository_block = True\n",
    "            repository_lines = [line]\n",
    "        elif in_repository_block and line.startswith(\"Clone URL:\"):\n",
    "            repository_lines.append(line)\n",
    "        elif in_repository_block and line.startswith(\"No keywords found in README\"):\n",
    "            repository_lines.append(line)\n",
    "            output_file.write(\"\\n\".join(repository_lines) + \"\\n\")\n",
    "            in_repository_block = False\n",
    "        elif not line:\n",
    "            in_repository_block = False\n",
    "\n",
    "# Display success message\n",
    "print(f\"Repositories with no keywords saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open issues in tensorflow/tensorflow:\n",
      "Issue #62719: ability to crete mask from points file\n",
      "  Created by: SigireddyBalasai\n",
      "  Created at: 2024-01-02T10:48:14Z\n",
      "  Labels: ['type:feature', 'type:docs-feature']\n",
      "\n",
      "Issue #62718: Critical typo: squeeze the y_pred tensor even when it’s the same rank and shape as the y_pred tensor should be == instead of !=\n",
      "  Created by: rodriguesk\n",
      "  Created at: 2024-01-02T08:52:31Z\n",
      "  Labels: ['type:docs-bug', 'stat:awaiting response']\n",
      "\n",
      "Issue #62717: Are the TFLite examples preventing color information from being passed along for inference?\n",
      "  Created by: titanium-cranium\n",
      "  Created at: 2024-01-02T05:59:13Z\n",
      "  Labels: ['comp:lite']\n",
      "\n",
      "Issue #62716: Are the TFLite examples preventing color information from being passed along for inference?\n",
      "  Created by: titanium-cranium\n",
      "  Created at: 2024-01-02T05:57:11Z\n",
      "  Labels: []\n",
      "\n",
      "Issue #62714: Fixed multiple typos\n",
      "  Created by: Cassini-chris\n",
      "  Created at: 2023-12-31T15:43:18Z\n",
      "  Labels: ['awaiting review', 'ready to pull', 'size:XS']\n",
      "\n",
      "Issue #62711: Tensorflow 2.13.1 not detecting GPU on Jetson Orin, not built with CUDA\n",
      "  Created by: Paul-Mick\n",
      "  Created at: 2023-12-30T17:58:54Z\n",
      "  Labels: ['stat:awaiting response', 'type:build/install', 'type:support', 'subtype: ubuntu/linux']\n",
      "\n",
      "Issue #62710: OOM when create a TensorShape {0, 16,16}\n",
      "  Created by: xuesu\n",
      "  Created at: 2023-12-30T12:51:12Z\n",
      "  Labels: ['type:bug', 'comp:runtime', 'TF2.14']\n",
      "\n",
      "Issue #62709: After converting the model to TFLite, there is a significant loss in accuracy.\n",
      "  Created by: panhu\n",
      "  Created at: 2023-12-30T10:55:39Z\n",
      "  Labels: ['comp:lite', 'TFLiteConverter']\n",
      "\n",
      "Issue #62708: Tnsor\n",
      "  Created by: Fergibson\n",
      "  Created at: 2023-12-30T09:20:05Z\n",
      "  Labels: ['invalid']\n",
      "\n",
      "Issue #62707: summarize_graph building failed\n",
      "  Created by: oldtimerzhy\n",
      "  Created at: 2023-12-29T03:31:59Z\n",
      "  Labels: ['type:bug']\n",
      "\n",
      "Issue #62705: TFLite CMake suggestion: option `TFLITE_ENABLE_GLES3` and changes for OpenGL ES Delegate build\n",
      "  Created by: luncliff\n",
      "  Created at: 2023-12-28T18:52:38Z\n",
      "  Labels: ['awaiting review', 'comp:lite', 'size:M']\n",
      "\n",
      "Issue #62704: how to implement movinet video action classification model in tensorflow c++ lite\n",
      "  Created by: ranjith502\n",
      "  Created at: 2023-12-28T17:39:41Z\n",
      "  Labels: ['stat:awaiting response', 'comp:lite-support']\n",
      "\n",
      "Issue #62703: TensorFlow在尝试将数据从CPU复制到GPU时遇到了问题\n",
      "  Created by: starxinyi\n",
      "  Created at: 2023-12-28T12:20:44Z\n",
      "  Labels: ['stat:awaiting response', 'type:build/install', 'subtype:windows', 'TF 2.10']\n",
      "\n",
      "Issue #62701: Model conversion crashed when feed data during quantization\n",
      "  Created by: houcheng\n",
      "  Created at: 2023-12-27T14:16:18Z\n",
      "  Labels: ['type:bug', 'comp:lite', 'TFLiteConverter', 'TF 2.15']\n",
      "\n",
      "Issue #62698: Add operator broadcast error\n",
      "  Created by: zxros10\n",
      "  Created at: 2023-12-27T03:46:57Z\n",
      "  Labels: ['stat:awaiting response', 'type:support', 'TFLiteConverter', 'TF 2.15']\n",
      "\n",
      "Issue #62697: Adding new feature to perform reverse operation of tf.image.extract_patches\n",
      "  Created by: ash-S26\n",
      "  Created at: 2023-12-26T17:36:13Z\n",
      "  Labels: ['type:feature']\n",
      "\n",
      "Issue #62696: the cuda version of my laptop is 12.1, but I can not find the correspond tensorflow-gpu\n",
      "  Created by: yoyoengineer\n",
      "  Created at: 2023-12-26T12:06:24Z\n",
      "  Labels: ['stat:awaiting response', 'type:build/install', 'type:support', 'comp:gpu', 'subtype:windows']\n",
      "\n",
      "Issue #62695: Lack of wheel file for tensorflow 1.x versions for aarch64 architecture machines.\n",
      "  Created by: akashKaitCaastle\n",
      "  Created at: 2023-12-26T11:53:29Z\n",
      "  Labels: ['type:feature', 'type:build/install', 'TF 1.15']\n",
      "\n",
      "Issue #62694: Fixing an issue with reflect padding in TfLite GPU delegate\n",
      "  Created by: mkarpushin-enhancelab\n",
      "  Created at: 2023-12-26T09:07:08Z\n",
      "  Labels: ['stat:awaiting response', 'comp:lite', 'size:XS']\n",
      "\n",
      "Issue #62693: I can't install tensorflow lower than 2.13\n",
      "  Created by: unolife\n",
      "  Created at: 2023-12-26T07:54:02Z\n",
      "  Labels: ['stat:awaiting response', 'type:build/install', 'subtype:macOS', 'TF 2.13']\n",
      "\n",
      "Issue #62692: Reduce TensorFlow Lite binary size(Build custom C/C++ shared libraries on android) For the models with the Select TF ops fail.\n",
      "  Created by: Qinlong275\n",
      "  Created at: 2023-12-26T02:20:55Z\n",
      "  Labels: ['type:bug', 'type:build/install', 'comp:lite', 'TF 2.12']\n",
      "\n",
      "Issue #62691: UnicodeDecodeError when file path of keras model contains an umlaut such as 'ü'\n",
      "  Created by: Steffenhir\n",
      "  Created at: 2023-12-25T10:34:25Z\n",
      "  Labels: ['type:bug', 'comp:keras', 'subtype:windows', 'TF 2.10']\n",
      "\n",
      "Issue #62690: How to get tf.data autotune metrics?\n",
      "  Created by: franklsf95\n",
      "  Created at: 2023-12-24T20:50:52Z\n",
      "  Labels: ['comp:data', 'type:performance', 'TF 2.15']\n",
      "\n",
      "Issue #62689: Unable to build TensorFlow without TensorRT\n",
      "  Created by: torotoki\n",
      "  Created at: 2023-12-24T07:39:04Z\n",
      "  Labels: ['type:build/install', 'subtype: ubuntu/linux']\n",
      "\n",
      "Issue #62687: Numerical precision of tf.math.cumsum operation on float32 vs float64\n",
      "  Created by: sapphire008\n",
      "  Created at: 2023-12-24T05:14:53Z\n",
      "  Labels: ['comp:ops', 'type:performance', 'TF 2.15']\n",
      "\n",
      "Issue #62686: Tensorflow Java 2.10.1 memory leak issue on constant creation\n",
      "  Created by: posuhov\n",
      "  Created at: 2023-12-23T22:00:44Z\n",
      "  Labels: ['stat:awaiting tensorflower', 'type:support', 'comp:core', 'TF 2.10']\n",
      "\n",
      "Issue #62685: TFLite cannot correctly recognize images\n",
      "  Created by: B-JackMao\n",
      "  Created at: 2023-12-23T10:34:36Z\n",
      "  Labels: ['stat:awaiting response', 'type:support', 'comp:lite', 'type:performance', 'TF 2.9']\n",
      "\n",
      "Issue #62684: Fix protobuf errors when using system protobuf\n",
      "  Created by: njzjz\n",
      "  Created at: 2023-12-23T06:20:11Z\n",
      "  Labels: ['ready to pull', 'size:XS']\n",
      "\n",
      "Issue #62683: model detects hand as a face. I cant retrieve confidence score from  tfjs-models/face-detection\n",
      "  Created by: DiannaShonia\n",
      "  Created at: 2023-12-22T13:50:35Z\n",
      "  Labels: ['stat:awaiting response', 'type:bug']\n",
      "\n",
      "Issue #62682: [AMD-ZENDNN] Enable CPU allocator for plugin\n",
      "  Created by: aadwived\n",
      "  Created at: 2023-12-22T12:22:20Z\n",
      "  Labels: ['ready to pull', 'size:XS', 'comp:core']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_open_issues(repo_owner, repo_name):\n",
    "    url = f'https://api.github.com/repos/{repo_owner}/{repo_name}/issues'\n",
    "    params = {'state': 'open'}\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        issues = response.json()\n",
    "        return issues\n",
    "    else:\n",
    "        print(f\"Failed to retrieve open issues. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "repo_owner = 'tensorflow'\n",
    "repo_name = 'tensorflow'\n",
    "open_issues = get_open_issues(repo_owner, repo_name)\n",
    "\n",
    "if open_issues:\n",
    "    print(f\"Open issues in {repo_owner}/{repo_name}:\")\n",
    "    for issue in open_issues:\n",
    "        print(f\"Issue #{issue['number']}: {issue['title']}\")\n",
    "        print(f\"  Created by: {issue['user']['login']}\")\n",
    "        print(f\"  Created at: {issue['created_at']}\")\n",
    "        print(f\"  Labels: {[label['name'] for label in issue['labels']]}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"Failed to retrieve open issues.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Repositories:\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def has_bug_issue(repository):\n",
    "    issues_url = f\"https://api.github.com/repos/{repository}/issues\"\n",
    "    params = {\"state\": \"all\"}\n",
    "\n",
    "    response = requests.get(issues_url, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        issues = response.json()\n",
    "        return any(\"bug\" in issue[\"title\"].lower() for issue in issues)\n",
    "        # or any(\"bug\" in label[\"name\"].lower() for label in issue[\"labels\"]) for issue in issues)\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def filter_repositories(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    repositories = []\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        if lines[i].startswith(\"Repository:\"):\n",
    "            repo_name = lines[i].split(\": \")[1].strip()\n",
    "            clone_url = lines[i + 1].split(\": \")[1].strip()\n",
    "            repositories.append((repo_name, clone_url))\n",
    "        i += 2\n",
    "\n",
    "    filtered_repositories = [\n",
    "        repo_name for repo_name, _ in repositories if has_bug_issue(repo_name)\n",
    "    ]\n",
    "\n",
    "    return filtered_repositories\n",
    "\n",
    "file_path = \"filtered_repos.txt\"\n",
    "filtered_repositories = filter_repositories(file_path)\n",
    "\n",
    "print(\"Filtered Repositories:\")\n",
    "for repo in filtered_repositories:\n",
    "    print(repo)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
